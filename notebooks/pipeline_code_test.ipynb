{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bea3d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from catboost import *\n",
    "from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b33fb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing():\n",
    "    \n",
    "    def __init__(self, filename = 'train_csv', df_type = 'classification', target = 'target', mode = 'train'):\n",
    "        \n",
    "        df = pd.read_csv(filename)\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.target = df[[target]]\n",
    "            df = df.drop(tagret, axis=1)\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        cols = df.columns\n",
    "        num_columns = list(self.df._get_numeric_data().columns)\n",
    "        cat_columns = list(set(cols) - set(num_cols))\n",
    "            \n",
    "        self.num_columns = num_columns\n",
    "        self.cat_columns = cat_columns\n",
    "        \n",
    "        print('numerical columns: ', len(num_columns))\n",
    "        print('categorical columns: ', len(cat_columns))\n",
    "        \n",
    "    \n",
    "    def feature_encoding(self):\n",
    "        \n",
    "        def feature_transform(self):\n",
    "            pass\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def fix_missed_values(self, method = 'pro', nn = 3):\n",
    "        \n",
    "        def missings(self):\n",
    "            return self.df.isna().sum().any()\n",
    "        \n",
    "        if not missings:\n",
    "            print('no missings here')\n",
    "            return \n",
    "        \n",
    "        match method:\n",
    "            case \"pro\":\n",
    "                #imputing numerical data\n",
    "                imputer = KNNImputer(n_neighbors=nn, weights=\"uniform\")\n",
    "                self.df[self.num_columns] = imputer.fit_transform(self.df[self.num_columns])\n",
    "                \n",
    "                #imputing categorical data, good only if count of categories not very big\n",
    "                dummies = pd.get_dummies(self.df)\n",
    "                imputer = KNNImputer(n_neighbors=nn, weights=\"uniform\")\n",
    "                dummies[:] = imputer.fit_transform(dummies)\n",
    "                data = pd.from_dummies(dummies, sep=\"_\")\n",
    "                self.df = data\n",
    "                \n",
    "                print('KNN imputing complete')\n",
    "            \n",
    "            case \"base\":\n",
    " \n",
    "                print('simple imputing complete')\n",
    "                \n",
    "            case \"mediana\":\n",
    "                \n",
    "                print('Missed data filling by mediana values complete')\n",
    "                \n",
    "            case \"mean\":\n",
    "                \n",
    "                print('Missed data filling by mean values complete')\n",
    "                \n",
    "            case \"mode\":\n",
    "                \n",
    "                print('Missed data filling by mode values complete')\n",
    "                \n",
    "            case _:\n",
    "                \n",
    "                print(\"incorrect method\")\n",
    "        \n",
    "\n",
    "    def data_scale(self):\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        self.df = pd.DataFrame(scaler.fit_transform(self.df), columns = self.df.columns)\n",
    "        \n",
    "    \n",
    "    def get_noisy_features(self):\n",
    "        \n",
    "        def create_models():\n",
    "            \n",
    "            cb = CatBoostClassifier(\n",
    "                iterations = 100,\n",
    "                max_depth = 7,\n",
    "                learning_rate = 0.07,\n",
    "                random_strength = 1,\n",
    "                l2_leaf_reg = 5,\n",
    "                random_state = 63,\n",
    "                verbose = False\n",
    "            )\n",
    "            \n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators = 300,\n",
    "                max_depth = 24,\n",
    "                criterion = 'gini',\n",
    "                n_jobs = -1,\n",
    "                random_state = 63,\n",
    "            )\n",
    "\n",
    "            lr = LogisticRegression(\n",
    "                solver = 'saga',\n",
    "                C = 0.5,\n",
    "            )\n",
    "\n",
    "            knn = KNeighborsClassifier(\n",
    "                n_neighbors = 15,\n",
    "                metric = 'euclidean',\n",
    "                weights = 'distance',\n",
    "            )\n",
    "\n",
    "            pc = MLPClassifier(\n",
    "                hidden_layer_sizes = 100,\n",
    "                activation = 'relu',\n",
    "                learning_rate = 'adaptive',\n",
    "                solver = 'adam',\n",
    "            )\n",
    "            \n",
    "            return [cb, rf, lr, knn, pc,]\n",
    "            \n",
    "        \"\"\"\n",
    "        generate noise to check features with importance lower than noises\n",
    "        df_with_noise\n",
    "        \"\"\"\n",
    "        \n",
    "        importances = dict()\n",
    "        \n",
    "        for model in create_models():\n",
    "            model.fit(df_with_noise, self.target)\n",
    "            importances[type(model)] = model.feature_importance()\n",
    "        \n",
    "        for model, f_importances in importances.items():\n",
    "            \"\"\"\n",
    "            check features with low importance\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        return features_to_delete\n",
    "    \n",
    "    \n",
    "    def delete_features():\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metamodel():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 xg_estimators = 50,\n",
    "                 cb_estimators = 100,\n",
    "                 lgbm_estimators = 50,\n",
    "                 rf_estimators = 300,\n",
    "                 max_depth_xg = 3,\n",
    "                 max_depth_cb = 3,\n",
    "                 max_depth_lgbm = 3,\n",
    "                 max_depth_rf = 3,\n",
    "                 lr_xg = 0.1,\n",
    "                 lr_cb = 0.1,\n",
    "                 lr_lgbm = 0.1,\n",
    "                 knneighs = 7,\n",
    "                 RS=63,\n",
    "                ):\n",
    "        \n",
    "        self.xgb = XGBClassifier(\n",
    "            n_estimators = xg_estimators,\n",
    "            max_depth = max_depth_xg,\n",
    "            colsample_bytree=0.5,\n",
    "            learning_rate=lr_xg,\n",
    "            alpha = 0.6,\n",
    "            random_state=RS,\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        self.cb = CatBoostClassifier(\n",
    "            iterations = cb_estimators,\n",
    "            max_depth = max_depth_cb,\n",
    "            learning_rate = lr_cb,\n",
    "            random_strength = 1,\n",
    "            l2_leaf_reg = 3,\n",
    "            random_state = RS,\n",
    "            verbose = False\n",
    "        )\n",
    "        \n",
    "        self.lgbm = LGBMClassifier(\n",
    "            n_estimators = lgbm_estimators,\n",
    "            objective = 'binary',\n",
    "            max_depth = max_depth_lgbm,\n",
    "            learning_rate = lr_lgbm,\n",
    "            reg_lambda = 0.3,\n",
    "            random_state = RS,\n",
    "            verbose = None\n",
    "        )\n",
    "        \n",
    "        self.rf = RandomForestClassifier(\n",
    "            n_estimators = rf_estimators,\n",
    "            max_depth = max_depth_rf,\n",
    "            criterion = 'gini',\n",
    "            n_jobs = -1,\n",
    "            random_state = RS,\n",
    "        )\n",
    "        \n",
    "        self.lr = LogisticRegression(\n",
    "            solver = 'saga',\n",
    "            C = 0.5,\n",
    "        )\n",
    "        \n",
    "        self.knn = KNeighborsClassifier(\n",
    "            n_neighbors = knneighs,\n",
    "            metric = 'euclidean',\n",
    "            weights = 'distance',\n",
    "        )\n",
    "        \n",
    "        self.pc = MLPClassifier(\n",
    "            hidden_layer_sizes = 150,\n",
    "            activation = 'relu',\n",
    "            learning_rate = 'adaptive',\n",
    "            solver = 'adam',\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.models = [self.xgb, self.cb, self.lgbm, self.rf, self.lr, self.knn, self.pc,]\n",
    "        \n",
    "        self.head = LogisticRegression(\n",
    "            solver = 'saga',\n",
    "            C = 0.5,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def new_head(self, model):\n",
    "        self.head = model\n",
    "        \n",
    "        \n",
    "    def fit_models(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            \n",
    "    def fit_head(self, X, y): # можно заменить predict на predict_proba что позволит мете работать лучше, но это не точно\n",
    "        \n",
    "        predicts = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            predicts.append(np.array(model.predict(X)))\n",
    "        \n",
    "        model_predicts = np.column_stack(predicts)\n",
    "        \n",
    "        self.head.fit(model_probas, y)\n",
    "        \n",
    "        return self.head.predict(model_predicts)\n",
    "    \n",
    "    \n",
    "    def meta_test(self, X_test):\n",
    "        \n",
    "        predicts = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            predicts.append(np.array(model.predict(X)))\n",
    "        \n",
    "        model_predict = np.column_stack(predicts)\n",
    "        \n",
    "        return np.array(self.head.predict(model_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
